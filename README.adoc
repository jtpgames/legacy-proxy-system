Legacy Proxy Pattern Experiments
:toc:
:toc-placement!:

This repository contains experimental scripts and automation tools for research on the Legacy Proxy Pattern, investigating its implementation, performance characteristics, and practical applications in software modernization.

toc::[]

== Overview
TODO

== Requirements

=== System Requirements

To enhance the reliability of our experimental results and ensure they robustly support our claims, we conducted the experiments across various hardware and software configurations. The specific configurations utilized by the authors include:

==== Configuration 1

* MacOS 10.15
* Homebrew package manager

==== Configuration 2
Virtual machine running with

* CPU: 16 vCPUs
* RAM: 32GB
* OS: Ubuntu 24.04 LTS 64-bit (newer OS versions are untested)

on the following hardware:
* CPU: 2x Intel® Xeon® Processor E5-2690 @ 2,90 GHz × 8

=== Dependencies

==== Bash Installation (MacOS)

The scripts require Bash 5.0 or higher. Install it via Homebrew:

[source,bash]
----
# Install latest bash version
brew install bash

# Replace buildin version of bash with the new version of bash
echo 'alias bash="/opt/homebrew/bin/bash"' >> ~/.zshrc
----

==== Bash Installation (Ubuntu)

The scripts require Bash 5.0 or higher. Ubuntu 24.04 comes with this version preinstalled.

==== Additional tools

[source,bash]
----
# Install ripgrep as a fast grep alternative
brew install ripgrep

----

[source,bash]
----
# -- From docker documentation --
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
--

sudo apt-get install -y software-properties-common build-essential bc ripgrep git docker-compose docker-ce screen python3.12-venv python3.12-dev

sudo groupadd docker
sudo usermod -aG docker $USER
----

== Troubleshooting

=== Network Delays Between Load Tester and Proxy Services

==== Problem Description

During load testing, intermittent delays of approximately 15 seconds were observed between the load tester sending requests and the first proxy service (ars-comp-1) receiving them. While the proxy services themselves processed requests efficiently (typically 40-50ms), these network-layer delays significantly impacted overall response times.

==== Analysis Process

1. **Request Tracing**: By analyzing load tester logs and proxy logs with matching request IDs, we confirmed that:
   - Load tester sends request at timestamp A
   - Proxy receives same request at timestamp A + 15 seconds
   - Proxy processes request normally in ~40ms

2. **Infrastructure Investigation**: Initial suspicions focused on:
   - Podman user-mode networking performance on macOS
   - FastAPI/Uvicorn configuration issues
   - Container resource limitations

3. **Root Cause Discovery**: The issue was traced to TCP listen queue overflow behavior:
   - System setting `net.ipv4.tcp_abort_on_overflow = 0` causes silent packet drops
   - When the listen queue is full, SYN packets are dropped rather than rejected
   - Clients experience exponential backoff delays (3s, 6s, 12s...) waiting for retransmission

==== Solution

Modify the TCP behavior in the Podman machine to immediately reject connections when the listen queue is full, rather than causing delays:

[source,bash]
----
# Temporary fix (active until VM restart)
podman machine ssh sudo sysctl -w net.ipv4.tcp_abort_on_overflow=1

# Permanent fix (persists across reboots)
podman machine ssh "echo 'net.ipv4.tcp_abort_on_overflow = 1' | sudo tee -a /etc/sysctl.conf"

# Verify the change
podman machine ssh sysctl net.ipv4.tcp_abort_on_overflow
----

==== Additional TCP Optimizations

For improved performance under high connection loads, apply these additional TCP optimizations:

[source,bash]
----
# Apply optimizations immediately
podman machine ssh "sudo sysctl -w net.core.netdev_max_backlog=2000"
podman machine ssh "sudo sysctl -w net.ipv4.tcp_max_syn_backlog=2048"
podman machine ssh "sudo sysctl -w net.ipv4.tcp_fin_timeout=30"
podman machine ssh "sudo sysctl -w net.ipv4.ip_local_port_range='1024 61000'"

# Make settings persistent across reboots
podman machine ssh "echo 'net.core.netdev_max_backlog = 2000' | sudo tee -a /etc/sysctl.conf"
podman machine ssh "echo 'net.ipv4.tcp_max_syn_backlog = 2048' | sudo tee -a /etc/sysctl.conf"
podman machine ssh "echo 'net.ipv4.tcp_fin_timeout = 30' | sudo tee -a /etc/sysctl.conf"
podman machine ssh "echo 'net.ipv4.ip_local_port_range = 1024 61000' | sudo tee -a /etc/sysctl.conf"
----

**Settings Explanation:**

* `net.core.netdev_max_backlog = 2000`: Increases network device packet queue size (default: 1000)
* `net.ipv4.tcp_max_syn_backlog = 2048`: Expands SYN packet backlog (default: 256)
* `net.ipv4.tcp_fin_timeout = 30`: Reduces connection cleanup time (default: 60 seconds)
* `net.ipv4.ip_local_port_range = 1024 61000`: Expands available port range for outbound connections

==== Docker Implementation

The TCP optimizations must also be applied at the container level, as Docker/Podman containers use separate network namespaces that do not inherit host-level sysctl settings. Both `docker-compose-legacy.yml` and `docker-compose-ng.yml` have been configured with container-level TCP optimizations.

**Container-Level Settings:**

All Python-based proxy containers now include the following sysctl configurations:

[source,yaml]
----
sysctls:
  - net.ipv4.tcp_max_syn_backlog=2048
  - net.ipv4.tcp_fin_timeout=30
  - net.ipv4.ip_local_port_range=1024 61000
  - net.ipv4.tcp_abort_on_overflow=1
----

**Note:** The `net.core.netdev_max_backlog` setting cannot be applied at the container level as it requires host-level privileges. This optimization remains effective at the Podman machine level for host-to-container traffic.

**Verification:**

To verify that container-level TCP settings are applied correctly:

[source,bash]
----
# Start a test container
cd python && docker-compose -f docker-compose-legacy.yml up -d ars-comp-1-1

# Check settings inside the container
docker exec <container-id> cat /proc/sys/net/ipv4/tcp_max_syn_backlog
docker exec <container-id> cat /proc/sys/net/ipv4/tcp_abort_on_overflow

# Clean up
docker-compose -f docker-compose-legacy.yml down
----

==== Fedora CoreOS Compatibility

The Podman machine environment uses Fedora CoreOS, which provides several advantages for TCP performance optimization:

**System Information:**
- **OS**: Fedora CoreOS 40.20241019.3.0
- **Kernel**: 6.11.3-200.fc40.aarch64
- **Architecture**: Container-optimized minimal OS

**Performance Benefits:**

* **Modern TCP Stack**: Kernel 6.11.3 includes latest TCP improvements and RFC implementations
* **Container-Optimized**: Designed specifically for containerized workloads with tuned networking defaults
* **Minimal Overhead**: Reduced background processes compared to desktop distributions
* **Immutable Design**: Persistent sysctl configurations survive reboots reliably

**Network Stack Features:**

[source,bash]
----
# Current congestion control (can be optimized further)
podman machine ssh "sysctl net.ipv4.tcp_congestion_control"
# Output: net.ipv4.tcp_congestion_control = cubic

# Kernel version verification
podman machine ssh "uname -r"
# Output: 6.11.3-200.fc40.aarch64
----

**Note**: The modern kernel and container-focused design of Fedora CoreOS provides an ideal foundation for the TCP optimizations implemented in this project. All host-level and container-level settings are fully compatible and effective.

**References:**

_Note: The following references were AI-generated and have not been fully reviewed for accuracy._

* Fedora CoreOS Documentation: https://docs.fedoraproject.org/en-US/fedora-coreos/[Official Fedora CoreOS Guide]
* Container optimization design: https://github.com/coreos/fedora-coreos-docs[Fedora CoreOS Architecture]
* Linux kernel 6.11 TCP improvements: https://kernelnewbies.org/Linux_6.11[Linux 6.11 Release Notes]
* Immutable OS benefits: https://ostree.readthedocs.io/[OSTree Documentation]

==== Alternative Solutions

If connection errors occur after the above change, consider increasing the application-level backlog:

[source,python]
----
# In FastAPI/Uvicorn applications
uvicorn.run(app, host=host, port=port, backlog=8192, ...)
----

==== References

_Note: The following references were AI-generated and have not been fully reviewed for accuracy._

* Linux TCP implementation: https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt[tcp_abort_on_overflow documentation]
* TCP listen queue behavior: Stevens, W. Richard. "TCP/IP Illustrated, Volume 1: The Protocols." Addison-Wesley, 2011.
* Uvicorn configuration: https://www.uvicorn.org/settings/#socket-bind[Uvicorn Settings Documentation]
* TCP optimization for high-performance servers: https://www.digitalocean.com/community/questions/max-number-of-concurrent-tcp-connections-to-droplet[DigitalOcean TCP Connection Limits]
* Linux network performance tuning: https://fasterdata.es.net/host-tuning/linux/[ESnet Linux Tuning Guide]
* TCP parameter tuning: Carder, Bradley. "Linux Network Performance Tuning." Linux Journal, 2016.
* High-performance networking: https://blog.packagecloud.io/monitoring-tuning-linux-networking-stack-receiving-data/[Linux Networking Stack Tuning]

=== Temporary DNS Resolution Errors in Performance Tests

==== Problem Description

During high-volume performance testing with hundreds of concurrent HTTP requests, temporary DNS resolution failures occurred when Python HTTP clients (httpx, requests) attempted to resolve Docker service names. These errors manifested as:

* **Error Type**: `socket.gaierror: [Errno -3] Temporary failure in name resolution`
* **Affected Services**: Load testers (Locust), ars-comp-1 proxies, ars-comp-2 proxies, and legacy-proxy-2 (MQTT-to-HTTP bridge)
* **Trigger**: High request rates (100+ RPS) with multiple concurrent connections to Docker service hostnames
* **Impact**: Request failures, test instability, and unreliable performance measurements

==== Root Cause Analysis

**Docker Embedded DNS Server Limitations**

Docker provides an *embedded DNS resolver* for containers on user-defined networks. Each container typically lists `127.0.0.11` as its nameserver, and the Docker engine forwards these queries to the host’s configured DNS servers. While this design simplifies service discovery, the embedded resolver introduces several operational constraints:

* *Finite Processing Capacity* – The resolver is implemented within the Docker engine (via _libnetwork_) and therefore has limited throughput. Under very high query rates or concurrent workloads, users have reported timeouts and dropped queries due to resource contention.
* *Limited Caching Behavior* – The resolver applies time-to-live (TTL) values to responses and caches entries for their duration, but it does not maintain an extensive cache across all lookups. This can result in repeated upstream queries and increased latency under load.
* *UDP-Dominant Transport* – Most DNS traffic in Docker networks uses UDP on port 53. Although DNS can fall back to TCP for truncated or large responses, the embedded resolver has exhibited degraded performance or timeouts in such cases, especially under high concurrency.
* *Shared Resource Model* – All containers attached to the same Docker network rely on the same embedded resolver instance on the host, making DNS resolution a centralized and potentially contended resource.

**Performance Test Characteristics**

Our experiments involve:

* **Service Topology**: Multi-tier architecture with services like `ars-comp-1-{1,2,3}`, `ars-comp-2-{1,2,3}`, `proxy1-{1,2,3}`, `proxy2-{1,2,3}`, and `ars-comp-3`
* **Request Volume**: Performance tests generate 100-500 requests per second
* **Connection Pattern**: Each request may trigger DNS lookups for service names like `ars-comp-2-1`, `ars-comp-2-2`, `mosquitto`, etc.
* **Concurrent Clients**: Multiple load testing workers and proxy instances making simultaneous requests

When combining high request rates with frequent service name resolution, the embedded DNS server can exceed its capacity, resulting in temporary failures.

==== Solution: Application-Level DNS Caching

To mitigate these DNS resolution failures, we implemented a **local DNS cache** at the application level in each Python service. This approach resolves hostnames to IP addresses once and caches the results in memory, eliminating repeated queries to the Docker DNS server.

**Implementation Details**

**Cache Structure:**
[source,python]
----
# DNS cache dictionary (hostname -> IP address)
dns_cache = {}

def resolve_hostname_to_ip(url: str) -> str:
    """Resolve hostname in URL to IP address, cache the result"""
    parsed = urlparse(url)
    hostname = parsed.hostname
    
    # Return original URL to disable caching (current configuration)
    return url
    
    # Caching logic (activated by removing the early return above)
    if hostname in dns_cache:
        logger.debug(f"DNS cache hit for {hostname} -> {dns_cache[hostname]}")
        return url.replace(hostname, dns_cache[hostname])
    
    try:
        ip_address = socket.gethostbyname(hostname)
        dns_cache[hostname] = ip_address
        logger.info(f"DNS resolved {hostname} -> {ip_address}")
        return url.replace(hostname, ip_address)
    except socket.gaierror as e:
        logger.warning(f"DNS resolution failed for {hostname}: {e}")
        return url
----

**Services with DNS Caching:**

* `locust_scripts/common/common_locust.py` - Load tester client (RepeatingHttpxClient)
* `python/ars_comp_1_proxy.py` - ARS Component 1 proxy
* `python/ars_comp_2_proxy.py` - ARS Component 2 proxy
* `python/mqtt/legacy_proxy_2.py` - MQTT-to-HTTP bridge

**Activation Mechanism:**

The DNS cache is currently **disabled by default** via an early `return url` statement in the `resolve_hostname_to_ip()` function. To enable caching for performance tests, remove or comment out this early return statement in the relevant Python files.

**Benefits:**

* **Reduced DNS Load**: Eliminates repeated lookups for the same hostname
* **Improved Reliability**: Prevents transient DNS failures under high load
* **Lower Latency**: Avoids DNS query overhead for cached hostnames
* **Consistent Performance**: Enables stable, repeatable performance test results

==== Failover Experiment Considerations

**DNS Cache Invalidation Issue**

For **failover experiments**, the local DNS cache must be **disabled**. When Docker containers are stopped and restarted during failover testing:

* **IP Address Changes**: Docker assigns a new IP address to the restarted container
* **Stale Cache Entries**: The DNS cache still contains the old IP address
* **Connection Failures**: Requests to cached IPs fail because the container is no longer reachable at that address

**Current Configuration:**

The DNS cache is disabled by default (via the early `return url` in `resolve_hostname_to_ip()`) to support failover experiments. This ensures that:

* DNS resolution happens on every request
* Container restarts with new IP addresses are handled correctly
* Failover behavior is tested accurately

**Recommended Approach:**

For different experiment types, consider these configurations:

|===
|Experiment Type |DNS Cache Status |Rationale

|Performance Tests
|**Enabled**
|Maximizes throughput and prevents DNS-related failures

|Failover Tests
|**Disabled** (default)
|Ensures proper handling of container restarts with new IPs

==== Docker Network DNS Architecture

**Service Discovery Mechanism:**

Docker Compose creates a default bridge network where each service is assigned:

* **Hostname**: The service name from docker-compose.yml (e.g., `ars-comp-1-1`, `proxy1-2`, `mosquitto`)
* **DNS Entry**: Automatically registered with the embedded DNS server at `127.0.0.11`
* **Dynamic IP**: Assigned from the bridge network subnet (e.g., `172.18.0.5`)

**Example Service Names from Our Configuration:**

**Legacy Pattern** (`docker-compose-legacy.yml`):
* `ars-comp-1-1`, `ars-comp-1-2`, `ars-comp-1-3` - Client-facing proxies
* `ars-comp-2-1`, `ars-comp-2-2`, `ars-comp-2-3` - Direct HTTP forwarding proxies
* `ars-comp-3` - RAST simulator

**New Generation Pattern** (`docker-compose-ng.yml`):
* `ars-comp-1-1`, `ars-comp-1-2`, `ars-comp-1-3` - Client-facing proxies
* `proxy1-1`, `proxy1-2`, `proxy1-3` - HTTP-to-MQTT bridges
* `proxy2-1`, `proxy2-2`, `proxy2-3` - MQTT-to-HTTP bridges
* `mosquitto` - MQTT broker
* `ars-comp-3` - RAST simulator

==== References

_Note: The following references were AI-generated and have not been fully reviewed for accuracy._

* https://docs.docker.com/network/bridge/#configure-dns[Docker Docs – Container networking / DNS services]
* https://github.com/moby/moby/issues/23430[Moby GitHub issue #23430]
* https://github.com/moby/moby/issues/31208[Moby GitHub issue #31208]
* https://github.com/moby/moby/issues/42680[Moby GitHub issue #42680]
* Serverfault forum: https://serverfault.com/questions/784495/poor-performance-with-docker-internal-dns

* Docker Embedded DNS: https://docs.docker.com/config/containers/container-networking/#dns-services[Docker DNS Services Documentation]
* Docker DNS Performance: https://github.com/moby/moby/issues/21424[Known DNS Resolution Issues Under Load]
* Python DNS Resolution: https://docs.python.org/3/library/socket.html#socket.gethostbyname[socket.gethostbyname() Documentation]
* DNS Caching Strategies: https://www.nginx.com/blog/dns-service-discovery-nginx-plus/[DNS Service Discovery and Caching Best Practices]
* Container Networking Performance: https://netdevconf.info/2.2/papers/gasparini-paper.pdf[Linux Container Networking Performance Analysis]
* Docker Network Architecture: https://success.docker.com/article/networking[Docker Reference Architecture: Networking]

== Usage

=== HTTP/2 Support with Granian

The proxy services in this project support HTTP/2 cleartext (h2c) connections through Granian, which provides performance benefits over HTTP/1.1 including request multiplexing and header compression.

==== Running Services with HTTP/2

The Docker Compose configurations (`docker-compose-legacy.yml` and `docker-compose-ng.yml`) already use the `start_python_app_with_tc.sh` script with the `--use-granian` flag to run all proxy services with HTTP/2 support:

[source,yaml]
----
# Services in docker-compose files use this format:
command: ./start_python_app_with_tc.sh ars_comp_1_proxy.py --use-granian
----

When you run the Docker Compose setup, all proxy services automatically start with Granian using HTTP/2 instead of the default Uvicorn with HTTP/1.1. The `--http 2` flag is automatically passed to Granian in the script.

To manually run a service with HTTP/2 support outside of Docker Compose:

[source,bash]
----
# Run a proxy service with HTTP/2 support using Granian
./start_python_app_with_tc.sh ars_comp_1_proxy.py --use-granian
----

==== Client-Side HTTP/2 Configuration

When httpx clients communicate with HTTP/2 services, they must be configured to use HTTP/2 exclusively. This is achieved by setting both `http2=True` and `http1=False` parameters when constructing the client:

[source,python]
----
import httpx

# For synchronous clients
client = httpx.Client(http2=True, http1=False)

# For asynchronous clients
async_client = httpx.AsyncClient(http2=True, http1=False)
----

**Important**: Without the `http1=False` parameter, httpx may attempt HTTP/1.1 negotiation, which can result in protocol errors like "illegal request line" when communicating with HTTP/2 services.

==== USE_HTTP_2 Environment Variable

The httpx clients in proxy services can be configured to use HTTP/2 via the `USE_HTTP_2` environment variable. This provides fine-grained control over which client connections use HTTP/2:

[source,yaml]
----
# In docker-compose files
environment:
  - USE_HTTP_2=true  # Enable HTTP/2 for httpx client
----

When `USE_HTTP_2` is set to `true`, `1`, or `yes`, the httpx client will be configured with:
* `http2=True` - Enable HTTP/2 protocol
* `http1=False` - Disable HTTP/1.1 to force HTTP/2 cleartext (h2c)

This setting controls the **client-side** HTTP protocol used for outbound requests, independent of how the service itself accepts incoming connections (which is controlled by the `--use-granian` flag).

==== Request Flow Diagrams

**Baseline Architecture:**

[source]
----
┌──────────────┐                 ┌─────────────────┐                 ┌─────────────────┐                 ┌──────────────┐
│   Locust     │    HTTP/2 or    │  ars-comp-1     │    HTTP/2 or    │  ars-comp-2     │     HTTP/1.1    │     RAST     │
│ Load Tester  │────HTTP/1.1────▶│  (ARS SRV 1)    │────HTTP/1.1────▶│  (ARS SRV 2)    │────(forced)────▶│  Simulator   │
│              │  (USE_HTTP_2)   │                 │  (USE_HTTP_2)   │                 │                 │              │
└──────────────┘                 └─────────────────┘                 └─────────────────┘                 └──────────────┘
     │                                   │                                   │                                   │
     │                                   │                                   │                                   │
   Granian                            Granian                            Granian                               Ktor
  (--use-granian)                    (--use-granian)                    (--use-granian)                       (HTTP/1.1)
   HTTP/2 h2c                        HTTP/2 h2c                         HTTP/2 h2c                          
   or Uvicorn                        or Uvicorn                         or Uvicorn                          
   HTTP/1.1                          HTTP/1.1                           HTTP/1.1                            
                                                                                                           
   httpx Client                      httpx Client                      httpx Client                        
   HTTP/2 or HTTP/1.1                HTTP/2 or HTTP/1.1                HTTP/1.1 (forced)                   
   (USE_HTTP_2)                      (USE_HTTP_2)                      (ignores USE_HTTP_2)
----

**Refactored Architecture with LPS:**

[source]
----
┌──────────────┐                 ┌─────────────────┐                 ┌─────────────────┐      ┌──────────┐      ┌─────────────────┐                 ┌─────────────────┐                 ┌──────────────┐
│   Locust     │    HTTP/2 or    │  ars-comp-1     │    HTTP/2 or    │ legacy-proxy-1  │      │  MQTT    │      │ legacy-proxy-2  │    HTTP/2 or    │  ars-comp-2     │     HTTP/1.1    │     RAST     │
│ Load Tester  │────HTTP/1.1────▶│  (ARS SRV 1)    │────HTTP/1.1────▶│  (HTTP→MQTT)    │─────▶│  Broker  │─────▶│  (MQTT→HTTP)    │────HTTP/1.1────▶│  (ARS SRV 2)    │────(forced)────▶│  Simulator   │
│              │  (USE_HTTP_2)   │                 │  (USE_HTTP_2)   │                 │ QoS2 │ MQTTv5   │ QoS2 │                 │  (USE_HTTP_2)   │                 │                 │              │
└──────────────┘                 └─────────────────┘                 └─────────────────┘      └──────────┘      └─────────────────┘                 └─────────────────┘                 └──────────────┘
     │                                   │                                   │                                           │                                   │                                   │
     │                                   │                                   │                                           │                                   │                                   │
   Granian                            Granian                            Granian                                    No Server                          Granian                                 Ktor
   (--use-granian)                   (--use-granian)                   (--use-granian)                             (MQTT Client)                      (--use-granian)                         (HTTP/1.1)
   HTTP/2 h2c                        HTTP/2 h2c                        HTTP/2 h2c                                  aiomqtt                            HTTP/2 h2c                          
   or Uvicorn                        or Uvicorn                        or Uvicorn                                  MQTTv5                             or Uvicorn                          
   HTTP/1.1                          HTTP/1.1                          HTTP/1.1                                                                       HTTP/1.1                            
                                                                                                                   httpx Client                                                            
   httpx Client                      httpx Client                      aiomqtt                                     HTTP/2 or HTTP/1.1                 httpx Client                        
   HTTP/2 or HTTP/1.1                HTTP/2 or HTTP/1.1                MQTTv5                                      (USE_HTTP_2)                       HTTP/1.1 (forced)                   
   (USE_HTTP_2)                      (USE_HTTP_2)                      (publishes to MQTT)                                                            (ignores USE_HTTP_2)
----

**Protocol Notes:**

* **Server-side protocol** (incoming): Controlled by `--use-granian` flag (HTTP/2) or default Uvicorn (HTTP/1.1)
* **Client-side protocol** (outgoing): Controlled by `USE_HTTP_2` environment variable in httpx clients
* **ars-comp-2 exception**: Always uses HTTP/1.1 for RAST communication regardless of `USE_HTTP_2` setting
* **MQTT Bridge**: legacy-proxy-2 is an MQTT subscriber that initiates HTTP requests (no incoming HTTP server)

**Files Configured for HTTP/2:**

* `python/ars_comp_1_proxy.py` - Uses HTTP/2 for downstream requests (configured via `USE_HTTP_2`)
* `python/ars_comp_2_proxy.py` - **HTTP/2 DISABLED**: Uses HTTP/1.1 only for downstream requests to RAST simulator (hardcoded, does not respect `USE_HTTP_2`)
* `python/mqtt/legacy_proxy_2.py` - Uses HTTP/2 for downstream requests (configured via `USE_HTTP_2`)
* `locust_scripts/common/common_locust.py` - Load test client configured for HTTP/2 connections (configured via `USE_HTTP_2`)

== Cloning
If you want to work on the current version of the project, clone the repository and pull all git submodules with the following command. 

[source]
----
git clone https://github.com/jtpgames/legacy-proxy-pattern.git && cd legacy-proxy-pattern && ./pull_all_submodules.sh
----

== License

This project is licensed under the BSD License - see the LICENSE file for details.

== AI Disclosure

This project utilized Warp AI (warp.dev) as an AI assistant in its development process. Specifically:

* **Documentation**: Warp AI was used to help write and improve portions of this README.adoc file, including troubleshooting guides and technical explanations.
* **Code Development**: Warp AI served as a coding assistant for developing and debugging bash and Python scripts.

While AI tools provided valuable assistance, all code and documentation have been reviewed, tested, and validated by the author. The final implementation decisions, architectural choices, and experimental design originate from the author.


